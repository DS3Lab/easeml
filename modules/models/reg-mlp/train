#!/usr/bin/python

import argparse
import json
import numpy as np
import os

import joblib
from sklearn.neural_network import MLPRegressor
from sklearn.preprocessing import StandardScaler

import easemlschema.schema as sch
import easemlschema.dataset as ds

dir_path = os.path.dirname(os.path.realpath(__file__))

with open(os.path.join(dir_path, "schema-in.json")) as f:
    schemaIn = json.load(f)

with open(os.path.join(dir_path, "schema-out.json")) as f:
    schemaOut = json.load(f)

schIn = sch.Schema.load(schemaIn)
schOut = sch.Schema.load(schemaOut)

#schIn = sch.Schema.load(schema["input"])
#schOut = sch.Schema.load(schema["output"])

if __name__ == "__main__":

    description = "K-NN Classifier."
    parser = argparse.ArgumentParser(description=description)

    parser.add_argument("--data", required=True, help="directory containing input data")
    parser.add_argument("--config", required=True, help="config file")
    parser.add_argument("--output", required=True, help="directory where the memory will be dumped")
    parser.add_argument("--metadata", required=False, help="directory where the metadata will be dumped")

    args = parser.parse_args()

    datasetIn = ds.Dataset.load(os.path.join(args.data, "input"))
    datasetOut = ds.Dataset.load(os.path.join(args.data, "output"))

    # Infer schemas.
    srcSchemaIn = datasetIn.infer_schema()
    srcSchemaOut = datasetOut.infer_schema()

    matchSchemaIn = schIn.match(srcSchemaIn, build_matching=True)
    matchSchemaOut = schOut.match(srcSchemaOut, build_matching=True)

    inName = matchSchemaIn.nodes["s1"].src_name
    outname = matchSchemaOut.nodes["s1"].src_name

    X_vectors = []
    y_values = []
    for name in datasetIn.children:
        if isinstance(datasetIn.children[name], ds.Directory) and name in datasetOut.children:
            inValue = datasetIn.children[name].children[inName].data
            outValue = datasetOut.children[name].children[outname].data[0]

            X_vectors.append(inValue)
            y_values.append(outValue)

    X = np.stack(X_vectors)
    y = np.stack(y_values)

    # Load the config file.
    with open(args.config) as f:
        config = json.load(f)

    hidden_layer_sizes = config["hidden_layer_sizes"]
    activation = config["activation"]
    solver = config["solver"]

    # Build and fit scaler.
    scaler = StandardScaler()
    X = scaler.fit_transform(X)

    # Initialize model and fit.
    model = MLPRegressor(hidden_layer_sizes=hidden_layer_sizes, activation=activation, solver=solver, max_iter=200)
    model.fit(X, y)

    # Save model and scaler.
    joblib.dump(scaler, os.path.join(args.output, "scaler.bin"))
    joblib.dump(model, os.path.join(args.output, "model.bin"))
